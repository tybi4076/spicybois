---
title: "Lab10"
author: "Spicy Bois"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(stringr)

questions <- read_csv("Questions_trunc.csv")
answers <- read_csv("Answers_trunc.csv")
```

# Lab 10
##Timeliness of the Answer as a Feature (Team)
```{r}
merged <- answers %>%
  left_join(questions, c('ParentId' = 'Id')) 

merged <- merged %>% 
  mutate(total_time = CreationDate.x - CreationDate.y) %>%
  filter(total_time>0)


ggplot(data = merged, mapping = aes(y = Score.x, x = total_time)) +
  geom_point() +
  geom_smooth() +
  ylim(0, 100) +
  geom_vline(xintercept=604800, color = "red")+
  labs(x = "Time Since Question Was Posted", y = "Score of Answer", title = "How the Score of the answer is effected by time ellapsed since question was posted", caption = "red line indicates 1 week mark between when a question is asked and when it gets answered") +
  theme_bw()
question_count <- data %>%
  filter(Total_Score != 0, Total_Score >= Score) %>%
  mutate(count = str_count(Question, "[:blank:]")) %>%
  group_by(count) %>%
  summarise(avg_score = mean(Score / Total_Score * 100)) %>%
  arrange(desc(avg_score))

ggplot(question_count, aes(x = count, y = avg_score)) + geom_point() + geom_smooth(se = FALSE)
```
**Findings:** As clearly shown in the graph, most questions are answered within the first week, and the sooner the answer, the more likely the answer will have a higher score. After this week mark, there is less correlation between answer score and time for the question to be answered, and the timeliness of the answer has less of an effect on the score itself. 

##Individual Sections

### Does the time of day effect the score? (Tyler)
```{r}
q_time <- questions %>%
  separate(CreationDate, into = c('Date', 'Time'), sep = " ") %>%
  separate('Time', into = c('hour', 'min', 'sec'), sep = ':') %>%
  mutate(temp = ifelse(hour>=16 & hour<=19 , "Good Time", "Bad Time")) %>%
  mutate(s = ifelse(Score>=51, "High Score", "Low Score")) %>%
  select(-X7)

a_time <- answers %>%
  separate(CreationDate, into = c('Date', 'Time'), sep = " ") %>%
  separate('Time', into = c('hour', 'min', 'sec'), sep = ':') %>%
  mutate(temp = ifelse(hour>=16 & hour<=19 , "Good Time", "Bad Time")) %>%
  mutate(s = ifelse(Score>=51, "High Score", "Low Score")) %>%
  select(-X7)

ggplot(data = a_time) +
  geom_bar(mapping  = aes(temp, fill = temp)) +
  facet_wrap(.~s) +
  theme_dark() +
  labs(title = "Time of Day Posting Relation to Scores for Questions", caption = "A high score is above 50 \n A Good Time is between 4pm and 8pm", x = "Time of Day", fill = "Time of Day", y = "Count")

ggplot(data = q_time) +
  geom_bar(mapping  = aes(temp, fill = temp)) +
  facet_wrap(.~s) + 
  theme_dark() +
  labs(title = "Time of Day Posting Relation to Scores for Questions", caption = "A high score is above 50 \n A Good Time is between 4pm and 8pm", x = "Time of Day", fill = "Time of Day", y = "Count") 

```
**Findings:** What I found is that the time of day doesn't effect whether a post gets a high or a low score. This doesn't change either for a question or an answer as most posts are low scoring posts, the time of day doesn't effect the score significantly.

### Does code being included in questions/answers effect the user score? (Trent)
```{r}
q_code <- questions %>%
  mutate(code = str_detect(Body, "<code>")) %>%
  mutate(rank = case_when((Score>=50)~"high", ((Score<50)~"low"))) %>%
  mutate(rank.num = ifelse(Score>=50, 1, 0))

q_code_means <- q_code %>%
  group_by(code) %>% summarise(mean(rank.num))

q_code <- q_code %>% mutate(rank.fact = as.factor(rank.num))
levels(q_code$rank.fact) <- c('Low', 'High')

q_code_means

ggplot(data = q_code) +
  geom_bar(mapping = aes(rank, fill = rank)) +
  facet_wrap(~ code) +
  facet_wrap(~ code) +
  labs(title = "How a score of a question if effected by whether or not code is included", x = "Score", y = "Count", fill="Score", caption = "high represents questions with scores over 50 votes, and low represents questions with less than 50 votes. TRUE reprenents questions that had code attatched to them, and FALSE questions do not") 

#answers code

a_code <- answers %>%
  mutate(code = str_detect(Body, "<code>")) %>%
  mutate(rank = case_when((Score>=50)~"high", ((Score<50)~"low"))) %>%
  mutate(rank.num = ifelse(Score>=50, 1, 0))

a_code_means <- a_code %>%
  group_by(code) %>% summarise(mean(rank.num))

a_code <- a_code %>% mutate(rank.fact = as.factor(rank.num))
levels(q_code$rank.fact) <- c('Low', 'High')

a_code_means

ggplot(data = a_code) +
  geom_bar(mapping = aes(rank, fill = rank)) +
  facet_wrap(~ code) +
  labs(title = "How a score of a answer if effected by whether or not code is included", x = "Score", y = "Count", fill="Score", caption = "high represents answers with scores over 50 votes, and low represents answers with less than 50 votes. TRUE reprenents answers that had code attatched to them, and FALSE answers do not")
```
**Findings:** Clearly, while there are still several low scoring questions and answers that included code, it does make a difference in whether or not a question/answer has a high score if code is included. For example, roughly 6% of answers that had over 50 upvotes included code, whereas only 0.8% had over 50 upvotes that did not have code included. So while the effect is not hugely substantial, it can still make a difference in question/answer score to include code. 

### Does the word count of the Answer effects the score? (Logan)
```{r, message=FALSE}
answers <- read_csv("Answers_trunc.csv", skip = 1, col_names = c("Answer_Id", "Testee_Id", "Answered_Date", "Question_Id", "Score", "Answer", "X7"))
questions <- read_csv("Questions_trunc.csv", skip = 1, col_names = c("Question_Id", "Creator_Id", "Creation_Date", "Total_Score", "Question", "Correct_Answer", "X7"))

data <- 
  answerS %>%
  left_join(question, "Question_Id") %>% 
  mutate(Total_Time = Answered_Date - Creation_Date) %>%
  filter(Total_Time > 0) %>%
  select("Question_Id", "Answer_Id", "Creator_Id", "Testee_Id", "Creation_Date", "Answered_Date", "Total_Time", "Score", "Total_Score", "Question", "Answer", "Correct_Answer")

answer_count <- data %>%
  filter(Total_Score != 0, Total_Score >= Score, Score >= 0) %>%
  mutate(act_score = (100*Score/Total_Score), rank = case_when((act_score >= 50)~"high", ((act_score < 50)~"low")), count = str_count(Answer, "[:blank:]"))

ggplot(answer_count, aes(x = count, y = act_score, color = rank)) + 
  geom_smooth(se = FALSE) + 
  labs(title = "Answer Word Count vs. Average Score", x = "Answer Word Count", y = "Score", caption = "Removed Scores that did not make sence (i.e. negative scores)")
```

**Findings:** The graph above shows that to get the a high score, it is best to keep your answer betwwen 100-500 words long. Anything longer will severaly impact your score. If you are aiming to get a low score, say you don't know the answer, it is best to keep is aroun 250 words or above 1000 words. The best answer would be around 3000 words. 

### Does the word count of the Question effect the score? (Logan)
```{r}
question_count <- data %>%
  filter(Total_Score != 0, Total_Score >= Score, Score >= 0) %>%
  mutate(act_score = (100*Score/Total_Score), rank = case_when((act_score >= 50)~"high", ((act_score < 50)~"low")), count = str_count(Question, "[:blank:]"))

ggplot(question_count, aes(x = count, y = act_score, color = rank)) + 
  geom_smooth(se = FALSE) + 
  labs(title = "Question Word Count vs. Average Score", x = "Question Word Count", y = "Score", caption = "Removed Scores that did not make sence (i.e. negative scores)")
```

###Does the addition of additional questions or followup increase a question/answer's score? (Joe)

```{r}
answers <- read_csv("Answers_trunc.csv")
questions <- read_csv("Questions_trunc.csv")
install.packages("stringr")
```
```{r}
library(stringr)
library(tidyverse)
library(dplyr)
```
```{r}
questionmarks <- questions %>% 
  select('Body', 'Score') %>%
  mutate(questions = str_detect(Body, "`?`")) %>%
  mutate(score = case_when((Score>=41)~"high", ((Score<41)~"low")))

ggplot(data = questionmarks) +
  geom_bar(mapping = aes(score, fill = score)) +
  labs(title = "Effect of followup questions on scores", 
       x = "Score", y = "Count",
       fill="Score")
```
```{r}
clearup <- answers %>% 
  select('Body', 'Score') %>%
  mutate(answers = str_detect(Body, "`?`")) %>%
  mutate(score = case_when((Score>=41)~"high", ((Score<41)~"low")))

ggplot(data = clearup) +
  geom_bar(mapping = aes(score, fill = score)) +
  labs(title = "Effect of clarifying questions on scores", 
       x = "Score", y = "Count",
       fill="Score")
```

**Finding:** It seems that if the test maker wants the test takers to do their best, they shoud have a question's that has a word length of 20-25 words long. For the low scorers, it seems that the question length has no meaning to them.

##Team Reports

**Tyler:** For this lab I analyzed how the time of day effected the amount of high or low scores. I did this by first separating the DateTime variable into a date and a time, and then separating the time into hour, min, and second. After separting that I set time to be the start and end of popular times people are online and a score to be the dictator of a high or low score. I then used ifelse's to create new columns to for scores whether they were high or low and for times whether it was inside or outside the time frame.

**Tyler:** I analyzed the correlation between if a question/answer included code, and how that effected the score. I did this by using the string detect function to find answers that included code, and filtered them into boolian "high" and "low" variables, based on whether there score was over 50 or not. I used a geom_bar to present my findings.

**Logan:** I analyzed the effect that word length has on the score for both the question and answer. To do this I first split the graph into the high and low scores to differacite between the top scores and those who did not understandt the question. I then counted the number of words in the questoin. I did this by counting the number of spaces. Then I ploted what I found.

**Joe** I looked at the effect that additional questions had on the scores of both the questions and the answers. I did this by finding a count of question marks in both the question and answer's "Body" column, then comparing that to the score that the question/answer of which the "Body" belonged to. I found that the scores were generall lower when there were additional questions in the bodies. I think that this is because people like simple questions and answers.


